{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Naive Bayes Classifier\n",
    "### Mihovil Mandic '21\n",
    "### COSC074 Winter 2019\n",
    "\n",
    "Notes:\n",
    "- Columns 1 through 6 of the given CSV file represent independent variables\n",
    "- The last column (“Label”) represents the dependent variable (0 or 1)\n",
    "- 3 functions:\n",
    "    - <strong>train(data)</strong>, where input data is the CSV filename of the training dataset, and output is a classifier C\n",
    "    - <strong>predict(C, row)</strong>, where input C is a classifier object on  train(...) was called, input row is an array corresponding to some row of IVs in the CSV test dataset (a single row only, to account for variable dataset sizes), output is 0 or 1.\n",
    "    - <strong>check(idx1, idx2)</strong>, cross-validates with dataframe limited to feature vectors from idx1 to idx2, and displays the accuracy and classification report\n",
    "    \n",
    "- <strong>I've selected 4 features for my classifier, Feature_2 to Feature_5; explained at the end of this notebook</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "input_file = \"hw1_trainingset.csv\"\n",
    "output_file = \"hw1_testset.csv\"\n",
    "trainset = pd.read_csv(input_file)\n",
    "testset = pd.read_csv(output_file)\n",
    "\n",
    "# Returns a trained classifier object\n",
    "def train(data):\n",
    "    gnb = GaussianNB()\n",
    "    X = data.loc[:, 'Feature_2':'Feature_5']    # Dropped Feature_1 and Feature_6 - explained below\n",
    "    y = data['Label']\n",
    "    gnb.fit(X, y)\n",
    "    return gnb\n",
    "\n",
    "# Takes in a classifier object and a dataframe row (2D array) as arguments\n",
    "# Returns the class label as an int64\n",
    "def predict(C, row):\n",
    "    return C.predict(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using functions to classify and writing the into final CSV file\n",
    "<b>Note: Created the new Label column with a default value of -1. This value is then changed row by row according to prediction (0 or 1)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled testset: \n",
      "      Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  Label\n",
      "0      13876.40  140984.00  -5240.000         33        315       5844      0\n",
      "1       9958.06   27101.60  -3330.730         27        174       1415      0\n",
      "2       8716.96   10783.70  -3113.440         34        156        654      0\n",
      "3      15707.20  116106.00  -5050.000         54        271       4426      0\n",
      "4       5189.89    1928.00  -1113.000         14         68        149      0\n",
      "5      15522.30  115720.00  -5350.000         47        271       4426      0\n",
      "6       7971.42   12121.00  -2564.000         27        110        765      0\n",
      "7      14985.50  140936.00  -4070.000         49        315       5844      0\n",
      "8       7471.31   13998.90  -2089.160         62        120        809      0\n",
      "9       4014.91    2176.70  -1127.700         20         54        165      0\n",
      "10      7297.55   14181.00  -3814.000          8        131        804      0\n",
      "11     15531.30  131051.00  -6548.000         59        353       5426      0\n",
      "12      6224.74    7286.13  -1591.470         50         93        545      0\n",
      "13      8794.65   29344.00  -3825.000         90        162       1576      0\n",
      "14      7928.00    9487.37  -1230.000         28        102        516      0\n",
      "15     12300.50   63094.00  -4707.000         67        269       2988      0\n",
      "16      9741.53   36221.10  -3770.000         41        196       1997      0\n",
      "17      7904.45   20182.60  -3025.140         67        145       1156      0\n",
      "18      8712.23   28876.30  -3332.860         37        179       1626      0\n",
      "19      8401.29   26143.60  -3130.000         25        156       1364      0\n",
      "20      4558.11    3980.58  -1379.140         10         72        340      0\n",
      "21     13006.20  154790.00  -6283.000        129        280       6508      1\n",
      "22      9454.18   26830.40  -3438.430         42        174       1415      0\n",
      "23      7092.56    9274.33  -3125.210         17        138        554      0\n",
      "24     11738.80   55357.00  -7532.000        163        231       2327      1\n",
      "25     20911.50  163006.00  -8730.000         93        449       5537      1\n",
      "26      8580.52   14183.00  -2421.000         28        118        845      0\n",
      "27      6297.16    5591.00  -2998.000         22        103        399      0\n",
      "28      8550.48   34661.00  -4331.000        102        171       1988      0\n",
      "29      4895.97    1879.96   -772.635         30         51        102      0\n",
      "...         ...        ...        ...        ...        ...        ...    ...\n",
      "2370    9593.41   27348.00  -5422.000         86        191       1522      0\n",
      "2371    9592.67   25927.60  -3330.620         23        174       1415      0\n",
      "2372   12498.30   73480.20  -4840.000         26        249       3630      0\n",
      "2373   13006.10   47127.70  -5839.020         58        273       2061      0\n",
      "2374    9912.71   26358.80  -2847.020         96        158       1403      0\n",
      "2375   23102.20  516465.00 -11454.700        136        540      18067      1\n",
      "2376    7414.79   13978.30  -2404.210         95        120        809      0\n",
      "2377    3529.01    2145.40  -1116.220         20         54        165      0\n",
      "2378   13197.70   60112.40  -4820.000         27        247       2754      0\n",
      "2379    5855.30    4718.89  -1290.000         23         85        343      0\n",
      "2380   10539.40   57689.40  -4488.480        138        217       2904      0\n",
      "2381   11162.60   30961.00  -4284.000         12        212       1365      0\n",
      "2382    8668.52   12739.90  -2589.410         91        136        619      0\n",
      "2383    6325.03    7802.57  -1693.640         67        104        457      0\n",
      "2384   15878.90  115820.00  -4380.000         59        271       4426      0\n",
      "2385   12703.50   56475.00  -4502.000         49        278       2752      0\n",
      "2386    9298.10   16407.00  -3554.000         83        171        849      0\n",
      "2387    7677.80    9436.20  -2946.740         98        130        490      0\n",
      "2388    8399.17   26423.00  -2220.000         27        156       1364      0\n",
      "2389    6991.69    7931.00  -3310.000         24        116        430      0\n",
      "2390    9852.88   10548.60  -2690.000         36        154        587      0\n",
      "2391   11140.60   49305.60  -4310.000         26        216       2677      0\n",
      "2392   18389.50  125450.00  -7255.000         75        363       4437      0\n",
      "2393   21557.50  284368.00 -16745.000         52        513       9696      1\n",
      "2394    8086.67   16670.00  -2812.320         81        142        844      0\n",
      "2395    6965.81    7481.82  -1490.000         21        111        453      0\n",
      "2396    8426.37   28769.00  -3465.090         36        179       1626      0\n",
      "2397    9785.12   17230.30  -3818.470         69        172        973      0\n",
      "2398    3951.19    2145.01  -1046.510         23         54        165      0\n",
      "2399    8910.83   20511.00  -3506.260         95        163       1162      0\n",
      "\n",
      "[2400 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "nrows = len(testset.index)    # 2400\n",
    "testset['Label'] = -1   # Default value for the new column\n",
    "\n",
    "# Classifier object\n",
    "clf = train(trainset)\n",
    "\n",
    "# Iterating over 2400 rows\n",
    "for i in range(nrows):\n",
    "    row = testset.iloc[i].values[1:5].reshape(1, -1)   # Looking at values in columns Feature_2 to Feature_5\n",
    "    testset.at[i, 'Label'] = predict(clf, row)    # Give the row to the predict function and save the value\n",
    "\n",
    "print('Labeled testset: ')\n",
    "print(testset)\n",
    "\n",
    "# Write into a new CSV file, without the row index column\n",
    "testset.to_csv(\"hw1_testset_classified.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Label frequency counts in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset - Label counts\n",
      "0    3230\n",
      "1    2370\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Testset - Label counts\n",
      "0    2050\n",
      "1     350\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainset - Label counts\")\n",
    "print(trainset['Label'].value_counts())\n",
    "print(\"\\nTestset - Label counts\")\n",
    "print(testset['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The difference in class ratios between the training set and test set is noticable. Perhaps the test set contained more variable data than the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking statistical parameters for both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature_1     Feature_2      Feature_3    Feature_4    Feature_5  \\\n",
      "count   5600.000000  5.600000e+03    5600.000000  5600.000000  5600.000000   \n",
      "mean   10971.668209  7.646888e+04   -4389.410329    58.234464   211.824107   \n",
      "std     5286.310454  1.280057e+05    6672.021675    48.152214   130.152914   \n",
      "min     2060.160000  4.222420e+01 -359930.000000     0.000000    33.000000   \n",
      "25%     7260.657500  9.782892e+03   -5660.960000    26.000000   119.000000   \n",
      "50%     9785.010000  3.191295e+04   -3368.850000    44.000000   180.000000   \n",
      "75%    13542.450000  8.334033e+04   -2159.777500    78.000000   271.000000   \n",
      "max    45110.200000  1.857700e+06  136000.000000   480.000000   981.000000   \n",
      "\n",
      "          Feature_6        Label  \n",
      "count   5600.000000  5600.000000  \n",
      "mean    2931.682500     0.423214  \n",
      "std     3856.844886     0.494113  \n",
      "min        5.000000     0.000000  \n",
      "25%      584.000000     0.000000  \n",
      "50%     1626.000000     0.000000  \n",
      "75%     3539.000000     1.000000  \n",
      "max    40369.000000     1.000000  \n",
      "          Feature_1     Feature_2     Feature_3    Feature_4    Feature_5  \\\n",
      "count   2400.000000  2.400000e+03  2.400000e+03  2400.000000  2400.000000   \n",
      "mean   10925.965063  7.533708e+04 -6.373875e+03    57.099583   211.245417   \n",
      "std     5244.944415  1.361997e+05  7.565591e+04    47.595205   129.967666   \n",
      "min     2242.170000  4.600850e+01 -2.730700e+06     0.000000    33.000000   \n",
      "25%     7244.660000  1.005970e+04 -5.551753e+03    25.000000   119.000000   \n",
      "50%     9832.720000  3.249515e+04 -3.369415e+03    43.000000   182.500000   \n",
      "75%    13426.425000  7.975445e+04 -2.160480e+03    77.000000   266.250000   \n",
      "max    50895.500000  2.352450e+06  7.530000e+04   506.000000   986.000000   \n",
      "\n",
      "          Feature_6        Label  \n",
      "count   2400.000000  2400.000000  \n",
      "mean    2883.075417     0.145833  \n",
      "std     3914.337284     0.353013  \n",
      "min        5.000000     0.000000  \n",
      "25%      587.000000     0.000000  \n",
      "50%     1581.000000     0.000000  \n",
      "75%     3484.000000     0.000000  \n",
      "max    40959.000000     1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(trainset.describe())\n",
    "print(testset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: I have noticed that Feature_3 in the testset is different than the one in the training set. The mean is much lower because the max is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "## *Appendix: Cross-validation, and accuracy check\n",
    "Without any feature selection, and by using <b>train_test_split</b> function for cross-validation on the training set, the accuracy was between 0.59 and 0.64, depending on the random_state for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.88      0.74       838\n",
      "           1       0.60      0.27      0.37       562\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1400\n",
      "   macro avg       0.62      0.57      0.55      1400\n",
      "weighted avg       0.62      0.63      0.59      1400\n",
      "\n",
      "Accuracy is: 0.6328571428571429\n"
     ]
    }
   ],
   "source": [
    "# Function that cross-validates the classifier, receieves from-to column indices as arguments\n",
    "def check(idx1, idx2):\n",
    "    X = trainset.columns[idx1:idx2] # features (all)\n",
    "    y = trainset.columns[-1] # target (class label - last column)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trainset[X], trainset[y], random_state=1)\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Accuracy is:', accuracy_score(y_test, y_pred))\n",
    "    return\n",
    "\n",
    "check(0, 6) # All feature columns 0 - 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "I am going to try and select K best features using the <b>SelectKBest</b> class offered by the scikit library.\n",
    "The scoring function used was <b>f_classif</b>, as I didn't want to manipulate the data because perhaps the negative values were important, and I didn't know what the features represent.\n",
    "\n",
    "In this part of the code I decided to check what the <b>k=4</b> best features were.\n",
    "The output from line 14 shows that the k=4 highest scoring features were Feature_2 to Feature_5 if we set the score threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores\n",
      "Feature_1: 70.56736208879721 \n",
      "Feature_2: 85.36832830778845 \n",
      "Feature_3: 134.48631971158613 \n",
      "Feature_4: 89.47029490159723 \n",
      "Feature_5: 107.79603861454493 \n",
      "Feature_6: 72.45654062902496 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.89      0.75       838\n",
      "           1       0.63      0.27      0.38       562\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1400\n",
      "   macro avg       0.64      0.58      0.56      1400\n",
      "weighted avg       0.64      0.64      0.60      1400\n",
      "\n",
      "Accuracy is: 0.6421428571428571\n"
     ]
    }
   ],
   "source": [
    "array = trainset.values\n",
    "X = array[:, 0:6]\n",
    "y = array[:, 6]\n",
    "\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, y)\n",
    "\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"Scores\")\n",
    "idx = 0\n",
    "for score in fit.scores_:\n",
    "    idx += 1\n",
    "    print(\"Feature_{0}: {1} \".format(idx, score))\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "# print(features[0:6,:])\n",
    "\n",
    "# check accuracy again, but drop lowest scored columns (0 and 6)\n",
    "check(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "Accuracy has slightly improved, and the precision for Label 1 has risen.\n",
    "Thus I will drop Feature_1 and Feature_6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
