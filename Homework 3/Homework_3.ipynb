{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - SVM\n",
    "### Mihovil Mandic, Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"CS74_HW4_training_set.csv\", sep=\",\")\n",
    "test_data = pd.read_csv(\"CS74_HW4_test_set.csv\", sep=\",\")\n",
    "new_test_data = test_data.copy()\n",
    "\n",
    "X_train = train_data.astype(float).iloc[:, 0:6]\n",
    "y_train = train_data.iloc[:, 6]\n",
    "\n",
    "# Uncomment the next line if you'd like to normalize the data\n",
    "#X_train = preprocessing.scale(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature_1     Feature_2     Feature_3    Feature_4    Feature_5  \\\n",
      "count   5600.000000  5.600000e+03  5.600000e+03  5600.000000  5600.000000   \n",
      "mean   10903.161545  7.444728e+04 -4.931251e+03    58.316607   209.785714   \n",
      "std     5147.732641  1.248326e+05  5.778578e+04    44.796549   127.243862   \n",
      "min     2017.360000  4.359050e+01 -3.396800e+06     0.000000    33.000000   \n",
      "25%     7521.820000  1.055090e+04 -5.411000e+03    27.000000   123.000000   \n",
      "50%     9722.810000  3.017500e+04 -3.330285e+03    45.000000   176.000000   \n",
      "75%    13320.000000  7.766975e+04 -2.260000e+03    79.000000   262.000000   \n",
      "max    45110.200000  1.566640e+06  1.410000e+06   450.000000   971.000000   \n",
      "\n",
      "          Feature_6        Label  \n",
      "count   5600.000000  5600.000000  \n",
      "mean    2890.982143     0.436071  \n",
      "std     3871.558555     0.495941  \n",
      "min        5.000000     0.000000  \n",
      "25%      621.000000     0.000000  \n",
      "50%     1523.000000     0.000000  \n",
      "75%     3450.000000     1.000000  \n",
      "max    37040.000000     1.000000   \n",
      "\n",
      "Training - Label counts\n",
      "0    0.563929\n",
      "1    0.436071\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.describe(), '\\n')\n",
    "\n",
    "print(\"Training - Label counts\")\n",
    "print(train_data['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature_1     Feature_2     Feature_3    Feature_4    Feature_5  \\\n",
      "count   2400.000000  2.400000e+03  2.400000e+03  2400.000000  2400.000000   \n",
      "mean   10808.028067  7.178885e+04 -5.009052e+03    56.897917   206.568750   \n",
      "std     5157.804265  1.227602e+05  5.125721e+04    44.635581   126.444024   \n",
      "min     1954.230000  4.741920e+01 -2.508800e+06     0.000000    33.000000   \n",
      "25%     7318.457500  1.014838e+04 -5.220000e+03    25.000000   119.000000   \n",
      "50%     9655.080000  2.875425e+04 -3.201320e+03    44.000000   174.000000   \n",
      "75%    13203.100000  7.349465e+04 -2.120750e+03    78.000000   254.000000   \n",
      "max    37483.900000  1.332080e+06  7.530000e+04   350.000000   954.000000   \n",
      "\n",
      "          Feature_6  \n",
      "count   2400.000000  \n",
      "mean    2788.262083  \n",
      "std     3789.484141  \n",
      "min        5.000000  \n",
      "25%      610.250000  \n",
      "50%     1519.000000  \n",
      "75%     3295.000000  \n",
      "max    35169.000000   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_data.describe(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means among all features seem the same or pretty similar between the train and the test set. Thus, the classifier should perform equally. The classes are a bit imbalanced - 56:44 ratio.\n",
    "\n",
    "The training data given isn't normalized, and scaling to (-1, 1) made sense. I have tried scaling and it left a minor impact on runtime (mostly helpful when grid searching with polynomial kernels), however it required modifying my ranges for C and gamma, so I decided not to normalize in the end.\n",
    "\n",
    "### SVM parameter tuning, 10-fold cross-validation\n",
    "\n",
    "Here I created a function `svc_param_tuning`.\n",
    "\n",
    "**Input**: X, y, and the number of folds for cross-validation.\n",
    "\n",
    "**Output**: Dict of best possible values for param_grid.\n",
    "\n",
    "I have started grid searching with \n",
    "`C = [1e-3, 1e-2, 1e-1, 1, 1e2, 1e3, 1e4]`.\n",
    "After that step I noticed that I was getting the best results with C in the range of (1, 10).\n",
    "Thus I modified to: `C = [1e-3, 1e-2, 1e-1, 1, 3, 5, 5.5, 6, 7, 10]` and narrowed it down to the range between (2, 5).\n",
    "It made sense to keep testing so I ended up my grid search by using `C = [1, 2, 2.4, 2.5, 3, 5, 10]`.\n",
    "\n",
    "Meanwhile gammas were always of negative power of 10, `gammas = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-1]`\n",
    "\n",
    "The best two values for C were `C = 2.4` and `C = 3`, but after a 10-fold CV, `C = 3` ended up with a higher score.\n",
    "\n",
    "I imported the sklearn class **GridSearchCV** as it seemed like the cleanest solution and it allowed a custom cv fold parameter. It does all the cross-validation + gridsearching across multiple parameters for me.\n",
    "\n",
    "Out of the kernels tested, `rbf` or `Radial Basis Function` always produced the most consistent and best results in the 0.72 - 0.77 range (scoring = `accuracy`). \n",
    "\n",
    "`poly` and `sigmoid` took a hefty amount of time to run, but I was never able to produce accurate results (compared to `rbf`) even when increasing `coef0` and `degrees`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.7675\n",
      "{'C': 3, 'gamma': 1e-06, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "def svm_param_tuning(X, y, k_folds):\n",
    "    #Cs = [1e-3, 1e-2, 1e-1, 1, 3, 5, 5.5, 6, 7, 10]\n",
    "    Cs = [1, 2, 2.4, 2.5, 3, 5, 10]\n",
    "    \n",
    "    gammas = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-1]\n",
    "    \n",
    "    #kernels = ['rbf', 'poly', 'sigmoid']\n",
    "    kernels = ['rbf']\n",
    "    \n",
    "    #degrees = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    #param_grid = {'C': Cs, 'gamma' : gammas, 'kernel' : kernels, 'degree' : degrees}\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas, 'kernel' : kernels}\n",
    "    \n",
    "    grid_search = GridSearchCV(svm.SVC(cache_size=20000), param_grid, n_jobs=-1, cv=k_folds, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    print('Best accuracy: ', grid_search.best_score_)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "print(svm_param_tuning(X_train, y_train, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, after a 10-fold CV, it seems that the best parameters I found are `C = 3`, `gamma = 1e-06`, and `rbf` as the kernel function.\n",
    "\n",
    "I added the `cache_size` variable value to the SVC class because my machine has plenty of RAM. Feel free to decrease, the value is in MB. `n_jobs` also as it refers to threads.\n",
    "\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    clf = svm.SVC(C=3, gamma=1e-06, kernel='rbf', cache_size=20000)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "clf = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final 10-fold CV accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75935829 0.76292335 0.75357143 0.78392857 0.76785714 0.79642857\n",
      " 0.77142857 0.72857143 0.7745975  0.7763864 ]\n",
      "Mean accuracy: 0.7675051254035521\n"
     ]
    }
   ],
   "source": [
    "accuracy = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "print(accuracy)\n",
    "print('Mean accuracy:', np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no feature selection or training data preprocessing the mean accuracy is roughly **76.75%** after a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction - 'hw3_test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf, test_data):\n",
    "    return clf.predict(test_data)\n",
    "    \n",
    "new_test_data['Label'] = predict(clf, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data - Label counts\n",
      "0    0.563929\n",
      "1    0.436071\n",
      "Name: Label, dtype: float64 \n",
      "\n",
      "Testing data - Label counts\n",
      "0    0.603333\n",
      "1    0.396667\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data - Label counts\")\n",
    "print(train_data['Label'].value_counts(normalize=True), '\\n')\n",
    "\n",
    "print(\"Testing data - Label counts\")\n",
    "print(new_test_data['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class proportions are a bit skewed when comparing to the training data, but as we don't know the ground truth, I believe that it's safe to assume that SVM helped us classify pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving into a new CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_data.to_csv(\"hw3_test_data_labeled.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
